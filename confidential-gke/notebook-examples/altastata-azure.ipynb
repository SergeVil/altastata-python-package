{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b49605-f356-41b5-b551-66e0ac12fb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from altastata.altastata_functions import AltaStataFunctions\n",
    "from altastata.altastata_pytorch_dataset import register_altastata_functions_for_pytorch\n",
    "\n",
    "# Configuration parameters\n",
    "user_properties = \"\"\"#bob123 account Azure\n",
    "#Wed Jul 16 10:55:59 EDT 2025\n",
    "sas-altastata-myorgrsa444-catalog-bob123=fGSnYF8mGb4THAQMzaI2tF+pRpTbcCdFXldBQcQFQBlWdDxSAdSUojAbYZHbtj3jgvlda9uaijCUkXAmtkyK+jB6w0XUx6CgdQRBHWyj7GUUnfl60oYthYhUzgvWNN9whqa4AWTQAuUGvSQEVoDS3SCSSBzoXEHBMk9d1zWyzg78g0G6c2ZvtI4QjnovuMXpOfbyZAhaejUxmZvt0Kvyua5rwEqn/Le/L2kFIVSXRsPFKLm+PKKEDic8PEv96hP532p2JRRko48nzwmeF9yAH3D+bzBsgPyc8yGoy2EJhhsA3LVpgtFFMPeErU7SEykwqn5cjAMX1Y+cEzWZgXnKWaTf8FU9RYvuHtIXdJcEatpz0Oev9jSxg4f6ZDwkrwcFyH/m\n",
    "sas-altastata-myorgrsa444-changes-bob123=1HWRTcGQcnT1UylGfTcNfToItnFNGnfHjvPeQCgBl/DSlAuBYLND3BEEIUz17CJCLJFmULSK8l+jRLGfmciU94FWsZXt+zXZ5IuITTOQ2wZ6aKYPIjN46a7CcQ1qNldn9xP3DqlgCh5hqooXCu9SsmAxqI1I/58fLBjlQ6g3VandrKe0NmJqXzrYZjTml6l7K5uof10VtQox341wzU6YrY2MUDfmgCVl/jcsbkfZW+u5FSQNnHbGlrvm5M/z6JFkqUau4uY5yQN3gvUR0emS0GyLKeMQWiWOwi/50oltgkQY7qazv5FUWkaZhmnSCHppuA3hNnjsCqEMaq2q4JdNCFA+40BxKFvPID0GZyA2TJ8hl9GmZCrVdD89pGouB0Oy2rvONeMGGA\\=\\=\n",
    "sas-altastata-myorgrsa444-chunks-bob123=70p1RHmfS9RKsJrFOxRmfz+UOumAahturdaC/Yheavl0c/P/mQqY7LRC2k3N4jo82lTqr+1eFA5DpwuFaW8I+VfhjpwBBo2n00SirvhiYBmYfhK1e0EqkLF7PEhfx4B05c+ys1ocu91xwTV7l6c75eHv7bDWuJWtUPkaCrOS1+oGjo6HIRioGITVXmMKG/W2nEUIc25OTV8xM5D8l5pnzQ9vkxpB8zXWdlq6yxFhdzqOSmZpy9cZJQP8mSTdgb1I149ZHaAchuqPxMsrvshf7a9jleza9wZvfIr4VRUATPBheEQ6im3uV4Xl/63RQRr10RZWQkKuqLxGa6tNsXZWZY4FBowAogyqjXDRNw0Ezt891A5S80jSrhL9FxqOwVkgRiOu\n",
    "sas-altastata-myorgrsa444-dataattributes-bob123=w/mDIsyKLuqY4r8QPC46+8w3800dBgbBQTfUliVI9V3uPOIebrItwAlSr2eWcTTjsPIfWjGfE2a6oLfoF4hEadZGoiY0ou/TEI8xFLOv2yaDAh9JpGCseqsZr6kTPNk09jygEKukI297itoyyyw3rIjAjwLmt89Wl59R5X2I15bSOUBUEPTfumRur5cxcsaRHOnVKbo8ZIehJulqtylgZksYn1RJsNQgJmT/I5dFp+/MKt6jZE+Hjt4i5aKn+OMi6Bm/49vXM6LYFtktuJv4123E9ut7N1ui2TGq6gkHDZ0952eGWTJMZqsnDAAzD+gcsf1SUE+ZcWCB2VV4N0U67tC3NlfEzjhDVE1NzRqXKC7Q2hlIPvducBJe9dNIzFY6anZ1bkE\\=\n",
    "myuser=bob123\n",
    "accounttype=azure-secure\n",
    "sas-altastata-myorgrsa444-bob123sqs=xdtcA0/fHlbwEHMqKBr2ZSaxfLExuY/5bJPL283uS1BkRnlcNa1xG+5pdaydRw7cb5Eppa7CKd0VMTZX1zwuM0YP6bwxihrMTdzd7KLJRp8QZZqcM01W5v60t6FUBWyUO6SAWWBNC5NyLQlro+r8aZd+adi8iVWa9UGNY2rtQmBKxdrJyczNatZZ/tFbKQlvRviT2S5L5XUhBHEmpZUB9nPPDuT7JDf5rMdbpiNh4wDOGFJV7VzgUkuve3flUcquxgKM59i3ahr6XjFfPBRjjKpf9jZEgjj7gNhdG4KmDHVDEmJ9OVpA4FW/29wmFZZqQbuTMFjtpFZFORyGe1I/Umt45G4ESFbCX6F93VlvEbgOLpx37UiIUf7Fc1P6og\\=\\=\n",
    "sas-altastata-myorgrsa444-users-all=ApFqou0AjkN1U4slOnjp7oQfXxli1uHr63nz9VzpeHwwkVO098IdPXS7HOT+GGDBa4vNG3CbPsfKn6vVytd6MfZjWb1Sn+LDSCgGDnaofXSGCpbggsKN9ichpgh+CcALnNgiVmKuFEtZuHcePnaQG02HfrF3Z0yK1o87dOEtQpbZOTzAFmmg1j7FA0Q6kbcG2MZB7ujI0FZ3Et+o67hhDEJ41ueNNBfu7WxacB4IFO+jCF9aj3NdCcpKp1czcg+bmjlpGc9R7qsWfo/VHFx7R2ukHtn91rseYPJHY39xkr42XbzXNIH3CNnKWKp3cPpWogeXkpRlqSb1FvLl68wLTgEq4cJWEIfxuaE0kYl0UvlOmchtPtgJ4MVKA+SF1Ag\\=\n",
    "metadata-encryption=RSA\n",
    "acccontainer-prefix=altastata-myorgrsa444-\n",
    "azure-account=https\\://sergeaccount.blob.core.windows.net\n",
    "\"\"\"\n",
    "\n",
    "private_key = \"\"\"-----BEGIN RSA PRIVATE KEY-----\n",
    "Proc-Type: 4,ENCRYPTED\n",
    "DEK-Info: AES-256-CBC,597B6162C9B305C49469A55D5250745E\n",
    "\n",
    "kDgpv6CrIej6vMvmg7JA2b3mb2z0kY3SZV7qERdHQ1ZAAarRwP0gzrVz7uGfNc22\n",
    "ZOuWAsbhBsJYU9fY32A7IP7jOPoa89E+9coDTxOOVFdg1Taz8jflnBSMxWvanWBD\n",
    "IpLRHikPTQlE9KVqG6pFeVZ0Iphm3BTL6MXW2PYvuwsLwjmp44DkbYIx7OYJykXe\n",
    "fWhjrRn5G+xWiwZqiRb0URtcdMF/0mcGxUic7v/8IcMD0z/BVnaJTU0j8nK4OOFp\n",
    "GA6yXhU4rhHqwr9LOuq5l0eZMOb9hDo87+qrFaN4o1h8zQCce5tZ8tI6pkUoD6lw\n",
    "37oCkfnFF6dA+gRdJARy9wd6vsB4LGOY70xDF0FpPFxAC/3MRR/PHuMQro225uxI\n",
    "5aJCeDm81tJ3gAJp7HeLwM+G8C6d3M3tVCtCHQfcdcS4TCMa/rfzpeAPx7CGL+VN\n",
    "n5Cjal4pvFjVIOKHlve+ovpcB86X125JFxBPvvcRD92krJ+IdKszEvGmhQ/P8NZ1\n",
    "hEy+EZ009GFvz2PrkUJf+fcz+i5M6UPmtfiw5s94B2H9ESKe1VuSTIcfM2dNetZS\n",
    "2dLtpcBChMThOZNRRoVvZfgsv/aW1gF0U9tQ4+HlpRn/xMk0qOupQSv3geIfkubj\n",
    "6QBEsZh1l3KnTiITS57YMpEr/uDLqJjZ6FkuoaOULEQzrARVnC56AfA875jQq1l+\n",
    "wHeL+BMgNvwBROZcM2iRDQtBXw+gSnb6w4Dzrat5ZbwQSFhIcoNqiYp3PGiUPTZN\n",
    "qYkXVDqfCzRSENu+2+7h4DKYDQW8brdmRTxZKm9YRFzFirO+JP5jG8wlpuaj53Fq\n",
    "-----END RSA PRIVATE KEY-----\"\"\"\n",
    "\n",
    "\n",
    "# Create an instance of AltaStataFunctions\n",
    "altastata_functions = AltaStataFunctions.from_credentials(user_properties, private_key)\n",
    "altastata_functions.set_password(\"123\")\n",
    "\n",
    "# register the altastata functions with PyTorch-specific registry\n",
    "register_altastata_functions_for_pytorch(altastata_functions, \"bob123_rsa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fc38b-ca3b-42fb-b7e8-814870f5ffaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset tests...\n",
      "==================================================\n",
      "\n",
      "Testing dataset with pattern: *.npy\n",
      "Root directory: pytorch_test/data/numpy\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/numpy/sample_0.npy✹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_1.npy✹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_2.npy✹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_3.npy✹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_4.npy✹bob123_1753279805338\n",
      "Number of files found: 5\n",
      "Data shape: torch.Size([2, 10, 5])\n",
      "Data type: torch.float32\n",
      "Data range: [0.000, 0.998]\n",
      "Labels: [0, 0]\n",
      "Test passed successfully!\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing dataset with pattern: *.png\n",
      "Root directory: pytorch_test/data/images\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_1.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_2.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_3.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_4.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_0.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_1.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_2.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_3.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_4.png✹bob123_1753279805338\n",
      "  pytorch_test/data/images/models\n",
      "Number of files found: 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from altastata import AltaStataPyTorchDataset\n",
    "\n",
    "def test_dataset_with_transforms(root_dir, pattern, expected_shape):\n",
    "    \"\"\"Test dataset with transforms and print results.\"\"\"\n",
    "    print(f\"\\nTesting dataset with pattern: {pattern}\")\n",
    "    print(f\"Root directory: {root_dir}\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Resize((224, 224))\n",
    "    ])\n",
    "    \n",
    "    dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=root_dir,\n",
    "        file_pattern=pattern,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    print(f\"Number of files found: {len(dataset)}\")\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=2, num_workers=0, shuffle=True)\n",
    "    batch_data, batch_labels = next(iter(dataloader))\n",
    "    \n",
    "    print(f\"Data shape: {batch_data.shape}\")\n",
    "    print(f\"Data type: {batch_data.dtype}\")\n",
    "    print(f\"Data range: [{batch_data.min().item():.3f}, {batch_data.max().item():.3f}]\")\n",
    "    print(f\"Labels: {batch_labels.tolist()}\")\n",
    "    \n",
    "    assert batch_data.shape == expected_shape, f\"Expected shape {expected_shape}, got {batch_data.shape}\"\n",
    "    print(\"Test passed successfully!\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"Starting dataset tests...\")\n",
    "print(\"=\" * 50)\n",
    "    \n",
    "# Test with numpy files (using new directory structure)\n",
    "test_dataset_with_transforms(\n",
    "    \"pytorch_test/data/numpy\",\n",
    "    \"*.npy\",\n",
    "    torch.Size([2, 10, 5])\n",
    ")\n",
    "    \n",
    "# Test with images (using new directory structure)\n",
    "test_dataset_with_transforms(\n",
    "    \"pytorch_test/data/images\",\n",
    "    \"*.png\",\n",
    "    torch.Size([2, 3, 224, 224])\n",
    ")\n",
    "    \n",
    "# Test with CSV files (using new directory structure)\n",
    "test_dataset_with_transforms(\n",
    "    \"pytorch_test/data/csv\",\n",
    "    \"*.csv\",\n",
    "    torch.Size([2, 11, 5])\n",
    ")\n",
    "    \n",
    "print(\"\\nAll tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45cc7011-8e9d-4c47-a1c7-af4b6c449b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.png✹bob123_1752670141265\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.png✹bob123_1752670141265\n",
      "\n",
      "Dataset Summary:\n",
      "Total files: 10\n",
      "Circle images: 5\n",
      "Rectangle images: 5\n",
      "\n",
      "Model summary:\n",
      "Total parameters: 9,533,442\n",
      "Model architecture: SimpleCNN with 3 conv blocks + classifier\n",
      "\n",
      "🚀 Starting PyTorch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: Val Loss: 0.0083, Val Acc: 100.0% ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 54. Best Val Loss: 0.0024\n",
      "\n",
      "⚡ PyTorch training completed in 12.7 seconds!\n",
      "\n",
      "Saving PyTorch model to AltaStata: pytorch_test/model/best_model.pth\n",
      "Saving model data length: 38148491 bytes\n",
      "Writing to cloud file: pytorch_test/model/best_model.pth\n",
      "Saving provenance file: pytorch_test/model/best_model.pth.provenance.txt with 10 file paths\n",
      "Writing to cloud file: pytorch_test/model/best_model.pth.provenance.txt\n",
      "\n",
      "Model info:\n",
      "Total parameters: 9,533,442\n",
      "Model saved successfully - ready for PyTorch inference! 🚀\n",
      "Provenance file saved: pytorch_test/model/best_model.pth.provenance.txt with 10 file paths\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from altastata import AltaStataPyTorchDataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 12 * 12, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, train_dataset, criterion, optimizer, num_epochs=100, patience=10):\n",
    "    \"\"\"Train the model with early stopping.\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n🚀 Starting PyTorch training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # Create progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training', \n",
    "                         leave=False, ascii=True)\n",
    "        \n",
    "        for images, labels in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            current_loss = train_loss / (train_pbar.n + 1)\n",
    "            current_acc = 100 * train_correct / train_total if train_total > 0 else 0\n",
    "            train_pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.1f}%'})\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Create progress bar for validation\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation', \n",
    "                           leave=False, ascii=True)\n",
    "            \n",
    "            for images, labels in val_pbar:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Update progress bar with current metrics\n",
    "                current_loss = val_loss / (val_pbar.n + 1)\n",
    "                current_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "                val_pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.1f}%'})\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "            # Only print significant improvements (>10% better) or milestones\n",
    "            if best_val_loss == val_loss and (epoch + 1) % 25 == 0:  # Every 25 epochs\n",
    "                print(f\"\\nEpoch {epoch + 1}: Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.1f}% ✓\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch + 1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"\\n⚡ PyTorch training completed in {training_time:.1f} seconds!\")\n",
    "    \n",
    "    # Load and save the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        # Save using the dataset with new directory structure\n",
    "        model_save_path = 'pytorch_test/model/best_model.pth'\n",
    "        print(f\"\\nSaving PyTorch model to AltaStata: {model_save_path}\")\n",
    "        train_dataset.save_model(best_model_state, model_save_path)\n",
    "        \n",
    "        # Model size info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nModel info:\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(\"Model saved successfully - ready for PyTorch inference! 🚀\")\n",
    "        print(f\"Provenance file saved: {model_save_path}.provenance.txt with {len(train_dataset.file_paths)} file paths\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "def train_model_main():\n",
    "    # Create transforms with data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create validation transform without augmentation\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets using AltaStataPyTorch with new directory structure\n",
    "    train_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/data/images\",  # Updated to new directory structure\n",
    "        file_pattern=\"*.png\",  # Pattern matches PNG files, excludes provenance\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/data/images\",  # Updated to new directory structure\n",
    "        file_pattern=\"*.png\",  # Pattern matches PNG files, excludes provenance\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Print dataset summary\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(f\"Total files: {len(train_dataset)}\")\n",
    "    circle_count = sum(1 for path in train_dataset.file_paths if 'circle' in str(path))\n",
    "    rectangle_count = sum(1 for path in train_dataset.file_paths if 'rectangle' in str(path))\n",
    "    print(f\"Circle images: {circle_count}\")\n",
    "    print(f\"Rectangle images: {rectangle_count}\\n\")\n",
    "    \n",
    "    # Create data indices for training and validation splits\n",
    "    dataset_size = len(train_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(0.2 * dataset_size))\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=4,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model summary:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Model architecture: SimpleCNN with 3 conv blocks + classifier\")\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, train_dataset, criterion, optimizer, num_epochs=100, patience=10)\n",
    "\n",
    "train_model_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f913e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2fcbc0-111c-4c13-892a-063dad16a86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/model/best_model.pth.provenance.txt✹bob123_1753125562560\n",
      "  pytorch_test/model/best_model.pth✹bob123_1753125558632\n",
      "Loaded model data length: 38148491 bytes\n",
      "Model loaded successfully!\n",
      "==================================================\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.png✹bob123_1752670141265\n",
      "\n",
      "Available files in dataset:\n",
      "  pytorch_test/data/images/circle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.png✹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.png✹bob123_1752670141265\n",
      "\n",
      "\n",
      "Image: circle_0.png\n",
      "True Label: Circle\n",
      "Predicted: Circle\n",
      "Confidence: 99.63%\n",
      "--------------------------------------------------\n",
      "\n",
      "Image: rectangle_0.png\n",
      "True Label: Rectangle\n",
      "Predicted: Rectangle\n",
      "Confidence: 99.78%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from altastata import AltaStataPyTorchDataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "# Configure matplotlib for Jupyter notebook if running in one\n",
    "try:\n",
    "    # Check if we're running in a Jupyter notebook\n",
    "    shell = get_ipython().__class__.__name__\n",
    "    if shell == 'ZMQInteractiveShell':  # Jupyter notebook or qtconsole\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    elif shell == 'TerminalInteractiveShell':  # IPython terminal\n",
    "        pass\n",
    "except:\n",
    "    pass  # Regular Python interpreter\n",
    "\n",
    "# Define the same model architecture as in training\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 12 * 12, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model.\"\"\"\n",
    "    model = SimpleCNN()\n",
    "    # Create dataset with file pattern to filter only .pth files (excludes provenance files)\n",
    "    model_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/model\",  # New directory structure\n",
    "        file_pattern=\"*.pth\",  # Pattern matches all .pth files, excludes .provenance.txt\n",
    "        require_files=False\n",
    "    )\n",
    "    \n",
    "    # Load using the resolved file path (pattern already filtered to .pth files)\n",
    "    model.load_state_dict(model_dataset.load_model(model_dataset.file_paths[0]))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def display_images(images, predictions, confidences):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(12, 6))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, image, pred, conf in zip(axes, images, predictions, confidences):\n",
    "        ax.imshow(np.array(image))\n",
    "        ax.set_title(f'Predicted: {\"Circle\" if pred == 1 else \"Rectangle\"}\\nConfidence: {conf:.2f}%')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def inference_example():\n",
    "    # Define transforms for inference (no augmentation needed)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_model(\"pytorch_test/model/best_model.pth\")\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create dataset for inference\n",
    "    test_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/data/images\",  # Updated to new directory structure\n",
    "        file_pattern=\"*.png\",  # Pattern matches PNG files, excludes provenance\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Print available files\n",
    "    print(\"\\nAvailable files in dataset:\")\n",
    "    for path in test_dataset.file_paths:\n",
    "        if isinstance(path, Path):\n",
    "            print(f\"  {path.name}\")\n",
    "        else:\n",
    "            # For cloud storage, print the full path\n",
    "            print(f\"  {path}\")\n",
    "    print()\n",
    "    \n",
    "    # Test on specific images\n",
    "    test_indices = [0, 5]  # circle_0.png and rectangle_0.png\n",
    "    \n",
    "    # Collect results for batch display\n",
    "    images = []\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for idx in test_indices:\n",
    "        # Get data and label from dataset\n",
    "        image_tensor, true_label = test_dataset[idx]\n",
    "        \n",
    "        # Get the original image\n",
    "        if isinstance(test_dataset.file_paths[idx], Path):\n",
    "            original_image = Image.open(test_dataset.file_paths[idx]).convert('RGB')\n",
    "        else:\n",
    "            # For cloud storage, use _read_file\n",
    "            image_bytes = test_dataset._read_file(test_dataset.file_paths[idx])\n",
    "            original_image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor.unsqueeze(0))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item() * 100\n",
    "        \n",
    "        # Store results\n",
    "        images.append(original_image)\n",
    "        predictions.append(predicted_class)\n",
    "        confidences.append(confidence)\n",
    "        \n",
    "        # Print results\n",
    "        file_path = test_dataset.file_paths[idx]\n",
    "        if isinstance(file_path, Path):\n",
    "            print(f\"\\nImage: {file_path.name}\")\n",
    "        else:\n",
    "            # For cloud storage, get the base filename\n",
    "            print(f\"\\nImage: {os.path.basename(file_path.split('✹')[0])}\")\n",
    "        print(f\"True Label: {'Circle' if true_label == 1 else 'Rectangle'}\")\n",
    "        print(f\"Predicted: {'Circle' if predicted_class == 1 else 'Rectangle'}\")\n",
    "        print(f\"Confidence: {confidence:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Display all images together\n",
    "    display_images(images, predictions, confidences)\n",
    "\n",
    "inference_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86f423-148e-4274-a010-e46ec7bf28b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
