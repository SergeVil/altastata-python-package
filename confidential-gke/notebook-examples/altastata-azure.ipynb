{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b49605-f356-41b5-b551-66e0ac12fb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from altastata.altastata_functions import AltaStataFunctions\n",
    "from altastata.altastata_pytorch_dataset import register_altastata_functions_for_pytorch\n",
    "\n",
    "# Configuration parameters\n",
    "user_properties = \"\"\"#bob123 account Azure\n",
    "#Wed Jul 16 10:55:59 EDT 2025\n",
    "sas-altastata-myorgrsa444-catalog-bob123=fGSnYF8mGb4THAQMzaI2tF+pRpTbcCdFXldBQcQFQBlWdDxSAdSUojAbYZHbtj3jgvlda9uaijCUkXAmtkyK+jB6w0XUx6CgdQRBHWyj7GUUnfl60oYthYhUzgvWNN9whqa4AWTQAuUGvSQEVoDS3SCSSBzoXEHBMk9d1zWyzg78g0G6c2ZvtI4QjnovuMXpOfbyZAhaejUxmZvt0Kvyua5rwEqn/Le/L2kFIVSXRsPFKLm+PKKEDic8PEv96hP532p2JRRko48nzwmeF9yAH3D+bzBsgPyc8yGoy2EJhhsA3LVpgtFFMPeErU7SEykwqn5cjAMX1Y+cEzWZgXnKWaTf8FU9RYvuHtIXdJcEatpz0Oev9jSxg4f6ZDwkrwcFyH/m\n",
    "sas-altastata-myorgrsa444-changes-bob123=1HWRTcGQcnT1UylGfTcNfToItnFNGnfHjvPeQCgBl/DSlAuBYLND3BEEIUz17CJCLJFmULSK8l+jRLGfmciU94FWsZXt+zXZ5IuITTOQ2wZ6aKYPIjN46a7CcQ1qNldn9xP3DqlgCh5hqooXCu9SsmAxqI1I/58fLBjlQ6g3VandrKe0NmJqXzrYZjTml6l7K5uof10VtQox341wzU6YrY2MUDfmgCVl/jcsbkfZW+u5FSQNnHbGlrvm5M/z6JFkqUau4uY5yQN3gvUR0emS0GyLKeMQWiWOwi/50oltgkQY7qazv5FUWkaZhmnSCHppuA3hNnjsCqEMaq2q4JdNCFA+40BxKFvPID0GZyA2TJ8hl9GmZCrVdD89pGouB0Oy2rvONeMGGA\\=\\=\n",
    "sas-altastata-myorgrsa444-chunks-bob123=70p1RHmfS9RKsJrFOxRmfz+UOumAahturdaC/Yheavl0c/P/mQqY7LRC2k3N4jo82lTqr+1eFA5DpwuFaW8I+VfhjpwBBo2n00SirvhiYBmYfhK1e0EqkLF7PEhfx4B05c+ys1ocu91xwTV7l6c75eHv7bDWuJWtUPkaCrOS1+oGjo6HIRioGITVXmMKG/W2nEUIc25OTV8xM5D8l5pnzQ9vkxpB8zXWdlq6yxFhdzqOSmZpy9cZJQP8mSTdgb1I149ZHaAchuqPxMsrvshf7a9jleza9wZvfIr4VRUATPBheEQ6im3uV4Xl/63RQRr10RZWQkKuqLxGa6tNsXZWZY4FBowAogyqjXDRNw0Ezt891A5S80jSrhL9FxqOwVkgRiOu\n",
    "sas-altastata-myorgrsa444-dataattributes-bob123=w/mDIsyKLuqY4r8QPC46+8w3800dBgbBQTfUliVI9V3uPOIebrItwAlSr2eWcTTjsPIfWjGfE2a6oLfoF4hEadZGoiY0ou/TEI8xFLOv2yaDAh9JpGCseqsZr6kTPNk09jygEKukI297itoyyyw3rIjAjwLmt89Wl59R5X2I15bSOUBUEPTfumRur5cxcsaRHOnVKbo8ZIehJulqtylgZksYn1RJsNQgJmT/I5dFp+/MKt6jZE+Hjt4i5aKn+OMi6Bm/49vXM6LYFtktuJv4123E9ut7N1ui2TGq6gkHDZ0952eGWTJMZqsnDAAzD+gcsf1SUE+ZcWCB2VV4N0U67tC3NlfEzjhDVE1NzRqXKC7Q2hlIPvducBJe9dNIzFY6anZ1bkE\\=\n",
    "myuser=bob123\n",
    "accounttype=azure-secure\n",
    "sas-altastata-myorgrsa444-bob123sqs=xdtcA0/fHlbwEHMqKBr2ZSaxfLExuY/5bJPL283uS1BkRnlcNa1xG+5pdaydRw7cb5Eppa7CKd0VMTZX1zwuM0YP6bwxihrMTdzd7KLJRp8QZZqcM01W5v60t6FUBWyUO6SAWWBNC5NyLQlro+r8aZd+adi8iVWa9UGNY2rtQmBKxdrJyczNatZZ/tFbKQlvRviT2S5L5XUhBHEmpZUB9nPPDuT7JDf5rMdbpiNh4wDOGFJV7VzgUkuve3flUcquxgKM59i3ahr6XjFfPBRjjKpf9jZEgjj7gNhdG4KmDHVDEmJ9OVpA4FW/29wmFZZqQbuTMFjtpFZFORyGe1I/Umt45G4ESFbCX6F93VlvEbgOLpx37UiIUf7Fc1P6og\\=\\=\n",
    "sas-altastata-myorgrsa444-users-all=ApFqou0AjkN1U4slOnjp7oQfXxli1uHr63nz9VzpeHwwkVO098IdPXS7HOT+GGDBa4vNG3CbPsfKn6vVytd6MfZjWb1Sn+LDSCgGDnaofXSGCpbggsKN9ichpgh+CcALnNgiVmKuFEtZuHcePnaQG02HfrF3Z0yK1o87dOEtQpbZOTzAFmmg1j7FA0Q6kbcG2MZB7ujI0FZ3Et+o67hhDEJ41ueNNBfu7WxacB4IFO+jCF9aj3NdCcpKp1czcg+bmjlpGc9R7qsWfo/VHFx7R2ukHtn91rseYPJHY39xkr42XbzXNIH3CNnKWKp3cPpWogeXkpRlqSb1FvLl68wLTgEq4cJWEIfxuaE0kYl0UvlOmchtPtgJ4MVKA+SF1Ag\\=\n",
    "metadata-encryption=RSA\n",
    "acccontainer-prefix=altastata-myorgrsa444-\n",
    "azure-account=https\\://sergeaccount.blob.core.windows.net\n",
    "\"\"\"\n",
    "\n",
    "private_key = \"\"\"-----BEGIN RSA PRIVATE KEY-----\n",
    "Proc-Type: 4,ENCRYPTED\n",
    "DEK-Info: AES-256-CBC,597B6162C9B305C49469A55D5250745E\n",
    "\n",
    "kDgpv6CrIej6vMvmg7JA2b3mb2z0kY3SZV7qERdHQ1ZAAarRwP0gzrVz7uGfNc22\n",
    "ZOuWAsbhBsJYU9fY32A7IP7jOPoa89E+9coDTxOOVFdg1Taz8jflnBSMxWvanWBD\n",
    "IpLRHikPTQlE9KVqG6pFeVZ0Iphm3BTL6MXW2PYvuwsLwjmp44DkbYIx7OYJykXe\n",
    "fWhjrRn5G+xWiwZqiRb0URtcdMF/0mcGxUic7v/8IcMD0z/BVnaJTU0j8nK4OOFp\n",
    "GA6yXhU4rhHqwr9LOuq5l0eZMOb9hDo87+qrFaN4o1h8zQCce5tZ8tI6pkUoD6lw\n",
    "37oCkfnFF6dA+gRdJARy9wd6vsB4LGOY70xDF0FpPFxAC/3MRR/PHuMQro225uxI\n",
    "5aJCeDm81tJ3gAJp7HeLwM+G8C6d3M3tVCtCHQfcdcS4TCMa/rfzpeAPx7CGL+VN\n",
    "n5Cjal4pvFjVIOKHlve+ovpcB86X125JFxBPvvcRD92krJ+IdKszEvGmhQ/P8NZ1\n",
    "hEy+EZ009GFvz2PrkUJf+fcz+i5M6UPmtfiw5s94B2H9ESKe1VuSTIcfM2dNetZS\n",
    "2dLtpcBChMThOZNRRoVvZfgsv/aW1gF0U9tQ4+HlpRn/xMk0qOupQSv3geIfkubj\n",
    "6QBEsZh1l3KnTiITS57YMpEr/uDLqJjZ6FkuoaOULEQzrARVnC56AfA875jQq1l+\n",
    "wHeL+BMgNvwBROZcM2iRDQtBXw+gSnb6w4Dzrat5ZbwQSFhIcoNqiYp3PGiUPTZN\n",
    "qYkXVDqfCzRSENu+2+7h4DKYDQW8brdmRTxZKm9YRFzFirO+JP5jG8wlpuaj53Fq\n",
    "-----END RSA PRIVATE KEY-----\"\"\"\n",
    "\n",
    "\n",
    "# Create an instance of AltaStataFunctions\n",
    "altastata_functions = AltaStataFunctions.from_credentials(user_properties, private_key)\n",
    "altastata_functions.set_password(\"123\")\n",
    "\n",
    "# register the altastata functions with PyTorch-specific registry\n",
    "register_altastata_functions_for_pytorch(altastata_functions, \"bob123_rsa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fc38b-ca3b-42fb-b7e8-814870f5ffaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset tests...\n",
      "==================================================\n",
      "\n",
      "Testing dataset with pattern: *.npy\n",
      "Root directory: pytorch_test/data/numpy\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/numpy/sample_0.npyâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_1.npyâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_2.npyâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_3.npyâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/numpy/sample_4.npyâœ¹bob123_1753279805338\n",
      "Number of files found: 5\n",
      "Data shape: torch.Size([2, 10, 5])\n",
      "Data type: torch.float32\n",
      "Data range: [0.000, 0.998]\n",
      "Labels: [0, 0]\n",
      "Test passed successfully!\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing dataset with pattern: *.png\n",
      "Root directory: pytorch_test/data/images\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_1.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_2.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_3.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/circle_4.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_0.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_1.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_2.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_3.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/rectangle_4.pngâœ¹bob123_1753279805338\n",
      "  pytorch_test/data/images/models\n",
      "Number of files found: 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from altastata import AltaStataPyTorchDataset\n",
    "\n",
    "def test_dataset_with_transforms(root_dir, pattern, expected_shape):\n",
    "    \"\"\"Test dataset with transforms and print results.\"\"\"\n",
    "    print(f\"\\nTesting dataset with pattern: {pattern}\")\n",
    "    print(f\"Root directory: {root_dir}\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Resize((224, 224))\n",
    "    ])\n",
    "    \n",
    "    dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=root_dir,\n",
    "        file_pattern=pattern,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    print(f\"Number of files found: {len(dataset)}\")\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=2, num_workers=0, shuffle=True)\n",
    "    batch_data, batch_labels = next(iter(dataloader))\n",
    "    \n",
    "    print(f\"Data shape: {batch_data.shape}\")\n",
    "    print(f\"Data type: {batch_data.dtype}\")\n",
    "    print(f\"Data range: [{batch_data.min().item():.3f}, {batch_data.max().item():.3f}]\")\n",
    "    print(f\"Labels: {batch_labels.tolist()}\")\n",
    "    \n",
    "    assert batch_data.shape == expected_shape, f\"Expected shape {expected_shape}, got {batch_data.shape}\"\n",
    "    print(\"Test passed successfully!\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"Starting dataset tests...\")\n",
    "print(\"=\" * 50)\n",
    "    \n",
    "# Test with numpy files (using new directory structure)\n",
    "test_dataset_with_transforms(\n",
    "    \"pytorch_test/data/numpy\",\n",
    "    \"*.npy\",\n",
    "    torch.Size([2, 10, 5])\n",
    ")\n",
    "    \n",
    "# Test with images (using new directory structure)\n",
    "test_dataset_with_transforms(\n",
    "    \"pytorch_test/data/images\",\n",
    "    \"*.png\",\n",
    "    torch.Size([2, 3, 224, 224])\n",
    ")\n",
    "    \n",
    "# Test with CSV files (using new directory structure)\n",
    "test_dataset_with_transforms(\n",
    "    \"pytorch_test/data/csv\",\n",
    "    \"*.csv\",\n",
    "    torch.Size([2, 11, 5])\n",
    ")\n",
    "    \n",
    "print(\"\\nAll tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45cc7011-8e9d-4c47-a1c7-af4b6c449b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.pngâœ¹bob123_1752670141265\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.pngâœ¹bob123_1752670141265\n",
      "\n",
      "Dataset Summary:\n",
      "Total files: 10\n",
      "Circle images: 5\n",
      "Rectangle images: 5\n",
      "\n",
      "Model summary:\n",
      "Total parameters: 9,533,442\n",
      "Model architecture: SimpleCNN with 3 conv blocks + classifier\n",
      "\n",
      "ðŸš€ Starting PyTorch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: Val Loss: 0.0083, Val Acc: 100.0% âœ“\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 54. Best Val Loss: 0.0024\n",
      "\n",
      "âš¡ PyTorch training completed in 12.7 seconds!\n",
      "\n",
      "Saving PyTorch model to AltaStata: pytorch_test/model/best_model.pth\n",
      "Saving model data length: 38148491 bytes\n",
      "Writing to cloud file: pytorch_test/model/best_model.pth\n",
      "Saving provenance file: pytorch_test/model/best_model.pth.provenance.txt with 10 file paths\n",
      "Writing to cloud file: pytorch_test/model/best_model.pth.provenance.txt\n",
      "\n",
      "Model info:\n",
      "Total parameters: 9,533,442\n",
      "Model saved successfully - ready for PyTorch inference! ðŸš€\n",
      "Provenance file saved: pytorch_test/model/best_model.pth.provenance.txt with 10 file paths\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from altastata import AltaStataPyTorchDataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 12 * 12, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, train_dataset, criterion, optimizer, num_epochs=100, patience=10):\n",
    "    \"\"\"Train the model with early stopping.\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\nðŸš€ Starting PyTorch training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # Create progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training', \n",
    "                         leave=False, ascii=True)\n",
    "        \n",
    "        for images, labels in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            current_loss = train_loss / (train_pbar.n + 1)\n",
    "            current_acc = 100 * train_correct / train_total if train_total > 0 else 0\n",
    "            train_pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.1f}%'})\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Create progress bar for validation\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation', \n",
    "                           leave=False, ascii=True)\n",
    "            \n",
    "            for images, labels in val_pbar:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Update progress bar with current metrics\n",
    "                current_loss = val_loss / (val_pbar.n + 1)\n",
    "                current_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "                val_pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.1f}%'})\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "            # Only print significant improvements (>10% better) or milestones\n",
    "            if best_val_loss == val_loss and (epoch + 1) % 25 == 0:  # Every 25 epochs\n",
    "                print(f\"\\nEpoch {epoch + 1}: Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.1f}% âœ“\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch + 1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"\\nâš¡ PyTorch training completed in {training_time:.1f} seconds!\")\n",
    "    \n",
    "    # Load and save the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        # Save using the dataset with new directory structure\n",
    "        model_save_path = 'pytorch_test/model/best_model.pth'\n",
    "        print(f\"\\nSaving PyTorch model to AltaStata: {model_save_path}\")\n",
    "        train_dataset.save_model(best_model_state, model_save_path)\n",
    "        \n",
    "        # Model size info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nModel info:\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(\"Model saved successfully - ready for PyTorch inference! ðŸš€\")\n",
    "        print(f\"Provenance file saved: {model_save_path}.provenance.txt with {len(train_dataset.file_paths)} file paths\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "def train_model_main():\n",
    "    # Create transforms with data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create validation transform without augmentation\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets using AltaStataPyTorch with new directory structure\n",
    "    train_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/data/images\",  # Updated to new directory structure\n",
    "        file_pattern=\"*.png\",  # Pattern matches PNG files, excludes provenance\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/data/images\",  # Updated to new directory structure\n",
    "        file_pattern=\"*.png\",  # Pattern matches PNG files, excludes provenance\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Print dataset summary\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(f\"Total files: {len(train_dataset)}\")\n",
    "    circle_count = sum(1 for path in train_dataset.file_paths if 'circle' in str(path))\n",
    "    rectangle_count = sum(1 for path in train_dataset.file_paths if 'rectangle' in str(path))\n",
    "    print(f\"Circle images: {circle_count}\")\n",
    "    print(f\"Rectangle images: {rectangle_count}\\n\")\n",
    "    \n",
    "    # Create data indices for training and validation splits\n",
    "    dataset_size = len(train_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(0.2 * dataset_size))\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=4,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model summary:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Model architecture: SimpleCNN with 3 conv blocks + classifier\")\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, train_dataset, criterion, optimizer, num_epochs=100, patience=10)\n",
    "\n",
    "train_model_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f913e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2fcbc0-111c-4c13-892a-063dad16a86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/model/best_model.pth.provenance.txtâœ¹bob123_1753125562560\n",
      "  pytorch_test/model/best_model.pthâœ¹bob123_1753125558632\n",
      "Loaded model data length: 38148491 bytes\n",
      "Model loaded successfully!\n",
      "==================================================\n",
      "account_id: bob123_rsa\n",
      "all_files:\n",
      "  pytorch_test/data/images/circle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.pngâœ¹bob123_1752670141265\n",
      "\n",
      "Available files in dataset:\n",
      "  pytorch_test/data/images/circle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/circle_4.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_0.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_1.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_2.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_3.pngâœ¹bob123_1752670141265\n",
      "  pytorch_test/data/images/rectangle_4.pngâœ¹bob123_1752670141265\n",
      "\n",
      "\n",
      "Image: circle_0.png\n",
      "True Label: Circle\n",
      "Predicted: Circle\n",
      "Confidence: 99.63%\n",
      "--------------------------------------------------\n",
      "\n",
      "Image: rectangle_0.png\n",
      "True Label: Rectangle\n",
      "Predicted: Rectangle\n",
      "Confidence: 99.78%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from altastata import AltaStataPyTorchDataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "# Configure matplotlib for Jupyter notebook if running in one\n",
    "try:\n",
    "    # Check if we're running in a Jupyter notebook\n",
    "    shell = get_ipython().__class__.__name__\n",
    "    if shell == 'ZMQInteractiveShell':  # Jupyter notebook or qtconsole\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    elif shell == 'TerminalInteractiveShell':  # IPython terminal\n",
    "        pass\n",
    "except:\n",
    "    pass  # Regular Python interpreter\n",
    "\n",
    "# Define the same model architecture as in training\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 12 * 12, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model.\"\"\"\n",
    "    model = SimpleCNN()\n",
    "    # Create dataset with file pattern to filter only .pth files (excludes provenance files)\n",
    "    model_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/model\",  # New directory structure\n",
    "        file_pattern=\"*.pth\",  # Pattern matches all .pth files, excludes .provenance.txt\n",
    "        require_files=False\n",
    "    )\n",
    "    \n",
    "    # Load using the resolved file path (pattern already filtered to .pth files)\n",
    "    model.load_state_dict(model_dataset.load_model(model_dataset.file_paths[0]))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def display_images(images, predictions, confidences):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(12, 6))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, image, pred, conf in zip(axes, images, predictions, confidences):\n",
    "        ax.imshow(np.array(image))\n",
    "        ax.set_title(f'Predicted: {\"Circle\" if pred == 1 else \"Rectangle\"}\\nConfidence: {conf:.2f}%')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def inference_example():\n",
    "    # Define transforms for inference (no augmentation needed)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_model(\"pytorch_test/model/best_model.pth\")\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create dataset for inference\n",
    "    test_dataset = AltaStataPyTorchDataset(\n",
    "        \"bob123_rsa\",\n",
    "        root_dir=\"pytorch_test/data/images\",  # Updated to new directory structure\n",
    "        file_pattern=\"*.png\",  # Pattern matches PNG files, excludes provenance\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Print available files\n",
    "    print(\"\\nAvailable files in dataset:\")\n",
    "    for path in test_dataset.file_paths:\n",
    "        if isinstance(path, Path):\n",
    "            print(f\"  {path.name}\")\n",
    "        else:\n",
    "            # For cloud storage, print the full path\n",
    "            print(f\"  {path}\")\n",
    "    print()\n",
    "    \n",
    "    # Test on specific images\n",
    "    test_indices = [0, 5]  # circle_0.png and rectangle_0.png\n",
    "    \n",
    "    # Collect results for batch display\n",
    "    images = []\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    for idx in test_indices:\n",
    "        # Get data and label from dataset\n",
    "        image_tensor, true_label = test_dataset[idx]\n",
    "        \n",
    "        # Get the original image\n",
    "        if isinstance(test_dataset.file_paths[idx], Path):\n",
    "            original_image = Image.open(test_dataset.file_paths[idx]).convert('RGB')\n",
    "        else:\n",
    "            # For cloud storage, use _read_file\n",
    "            image_bytes = test_dataset._read_file(test_dataset.file_paths[idx])\n",
    "            original_image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor.unsqueeze(0))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item() * 100\n",
    "        \n",
    "        # Store results\n",
    "        images.append(original_image)\n",
    "        predictions.append(predicted_class)\n",
    "        confidences.append(confidence)\n",
    "        \n",
    "        # Print results\n",
    "        file_path = test_dataset.file_paths[idx]\n",
    "        if isinstance(file_path, Path):\n",
    "            print(f\"\\nImage: {file_path.name}\")\n",
    "        else:\n",
    "            # For cloud storage, get the base filename\n",
    "            print(f\"\\nImage: {os.path.basename(file_path.split('âœ¹')[0])}\")\n",
    "        print(f\"True Label: {'Circle' if true_label == 1 else 'Rectangle'}\")\n",
    "        print(f\"Predicted: {'Circle' if predicted_class == 1 else 'Rectangle'}\")\n",
    "        print(f\"Confidence: {confidence:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Display all images together\n",
    "    display_images(images, predictions, confidences)\n",
    "\n",
    "inference_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86f423-148e-4274-a010-e46ec7bf28b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
